<!DOCTYPE html>
<html>
  <head>
	<title>Reality Capture</title>
	<link rel="stylesheet" href="style.css" type="text/css" />
  </head>
  <body>
  	<nav>
	<ul>
	<li><a href="index.html" alt="Future Cinema II">Future Cinema II</a></li>
<li><a href="log.html" alt="Log">Log</a></li>
<li><a href="vr.html" alt="Virtual Reality">Virtual Reality</a></li>
<li><a href="perception.html" alt="Perception">Perception</a></li>
<li><a href="notgame.html" alt="(not-)game">(not-)game</a></li>
<li><a href="osmose.html" alt="Osmose">Osmose</a></li>
<li><a href="reading.html" alt="Reading">Reading</a></li>
<li><a href="lab.html" alt="Lab">Lab</a></li>
<li><a href="unreal.html" alt="Unreal">Unreal</a></li>
<li><a href="max.html" alt="Max">Max</a></li>
<li><a href="real.html" alt="Reality Capture">Reality Capture</a></li>
	</ul>
	</nav>	
	<div id="main">
	<h1 id="getting-the-real-into-virtual-reality">Getting the real into virtual reality</h1>
<intro>

<h2 id="sound">Sound</h2>
<p>To a certain extent, bringing sound into a virtual world is not significantly different from the workflows known from film and gaming. Moreover, sound can add a tremendous amount to the sense of presence in a world. </p>
<p>One significant difference from conventional film is in the approach to <strong>spatialization</strong>: in VR we are immersed within a world, and sound should come from all around us, not only in front of us (and almost never from exactly where we are!)  There are spatialization methods to make it seem that audio comes from a different direction (e.g. ambisonics and head-related transfer functions), and from near or far (&quot;distance cues&quot;), whether a sound source is approaching or receding (via Doppler effects), and whether the space itself is resonant (reverberation). Unreal has some good support for spatializing sounds, and Max has some very excellent tools in this space. </p>
<p>Another important difference is in timing: if there is no single timeline, then audio mixes happen according to events in the world. This is the norm in video games, and often involves a combination of long-running tracks, short event-based sound cues, longer overlapping loops, and varying mix levels and effects in response to world events. Again, Unreal has some built-in support, while Max is far more powerful and open-ended in this regard. </p>
<p>Naturally, any recording will work better if taken without any background noise and in a non-reverberant space. </p>
<p>An open question is how to deal with non-diegetic audio, or even to what extent this makes sense.</p>
<h2 id="2d-images-and-video">2D Images and video</h2>
<p>It is very easy to bring a 2D image into a world, but unlike screen-based video, we cannot simply place it in a fixed location on screen. One of the oft-mentioned guidelines for VR is to never attach anything to the viewer&#39;s perspective. So any 2D images must instead exist as surfaces within the world. This means we can approach them, walk around them, etc. In computer graphics terms the images become textures on geometry meshes. However, although images are 2D, they don&#39;t necessarily need to be flat. In fact nearly all the things you can see in a typical video game or software VR world are image textures wrapped across 3D geometry. Images may also have transparency (opacity or &quot;alpha&quot; channels) that will make geometry transparent in the virtual world. </p>
<p>It is also possible to wrap a video source onto a geometry in the same way -- including with transparency. As with sound there needs to be sensitivity with regard to timing: triggering, looping, synchronizing etc.</p>
<h2 id="motion-capture">Motion Capture</h2>
<p>The term <strong>Motion Capture</strong> (AKA MoCap) is generally used to describe systems that capture the bodily movements of actors, in the form of animated &quot;skeletons&quot;, which can then be used to animate virtual characters (via &quot;rigging&quot; the captured skeleton to the skeleton of a character model). MoCap can be <strong>marker-based</strong>, where an actor wears a suit with markers (such as highly-reflective bobbles), or can be <strong>marker-less</strong>, where there are very few restrictions on what an actor wears. </p>
<p>Marker-based MoCap is also used to track rigid bodies, which are fixed arrangements of markers attached to objects that can also be accurately tracked. In making <em>Avatar</em>, James Cameron used marker-based tracking of a tablet PC as a virtual camera into a real-time rendered version of a scene while it was being acted. </p>
<iframe width="640" height="360" src="https://www.youtube.com/embed/OJ1JzYPjcj0?rel=0" frameborder="0" allowfullscreen></iframe>

<p>We have access to a markerless MoCap system (by Organic Motion) at the CineSpace facility.  The Microsoft Kinect also provides a form of markerless motion capture with its skeleton tracking capability, though it has a much smaller field of view and tracking area.</p>
<h2 id="photogrammetry">Photogrammetry</h2>
<p>The term <strong>Photogrammetry</strong> (AKA photo-scanning, and also structure-from-motion) refers to a set of algorithmic methods by which a set of 2D photographs, taken from different points of view, can be analyzed to produce a 3D model of the scene. It is useful for capturing real-world static 3D objects into 3D model assets that can be placed within a virtual world. It can also be used at a larger scale to capture 3D models of entire environments. </p>
<p>Photogrammetry has become widely used for bringing more realistic assets into videogames. For example, see <a href="http://starwars.ea.com/starwars/battlefront/news/how-we-used-photogrammetry">Star Wars Battlefront</a>. Here&#39;s a great example of an industrial site captured shortly before its demolition, and experienced in VR via the Unreal Engine:</p>
<iframe width="640" height="360" src="https://www.youtube.com/embed/DqraO04zbxI?rel=0" frameborder="0" allowfullscreen></iframe>

<p>Here&#39;s another example, captured using Agisoft and imported into Unreal, <a href="https://forums.unrealengine.com/showthread.php?66011-Unreal-Engine-4-Photogrammetry">described in this thread</a></p>
<iframe width="640" height="360" src="https://www.youtube.com/embed/LbJm-sWqpVw?rel=0" frameborder="0" allowfullscreen></iframe>

<p>Valve&#39;s <a href="http://store.steampowered.com/app/453170">Destinations</a> VR app can load photogrammetry scenes -- <a href="https://developer.valvesoftware.com/wiki/Destinations/Creating_a_Destination">here&#39;s a tutorial</a></p>
<h3 id="photogrammetry-from-video-and-other-image-sources">Photogrammetry from video and other image sources</h3>
<p>Typically photogrammetry works from a collection of photos of an object or site, but of course video can also serve this purpose, when the video camera is mobile. Creating photogrammetry from video requires long, continuous dolly/tracking type shots, of scenes with consistent lighting a no moving objects, and usually taken with the same camera settings (no zoom!). The artist Claire Hentschker has produced a series of interesting VR worlds derived primarily from photogrammetry of video, including from:</p>
<ul>
<li><a href="http://postmatter.com/galleries/new-mythologies/claire-hentschker/">YouTube videos of abandoned shopping malls</a>. <a href="https://www.youtube.com/watch?v=xeahnvnk2vU">Watch here</a></li>
<li>Kubrick&#39;s The Shining. <a href="https://www.youtube.com/watch?v=AupAFblRwgY">Watch here</a></li>
</ul>
<p>The US Navy has also explored photogrammetry from non-standard sources <a href="http://www.public.navy.mil/spawar/Pacific/Robotics/Documents/Publications/2015/SPIE_Harguess_SFM_2015.pdf">as described in this paper</a>, drawing source images from Youtube, from Google Earth, and from quadcopter drones. </p>
<h3 id="software">Software</h3>
<p><a href="http://www.regard3d.org"><strong>Regard3D</strong></a></p>
<ul>
<li>free &amp; open source, but sometimes overwhelming in options</li>
<li><a href="http://www.regard3d.org/index.php/documentation/tutorial">tutorial</a></li>
<li><a href="https://developer.valvesoftware.com/wiki/Destinations/Photogrammetry_with_Regard3D">more detailed tutorial</a></li>
</ul>
<p><a href="http://remake.autodesk.com/try-remake"><strong>Autodesk ReMake</strong></a></p>
<ul>
<li>free for students &amp; education, but limited quality</li>
<li><a href="http://autodeskremake.squarespace.com/blog/2016/5/31/how-to-take-photos-that-will-produce-best-3d-models">shooting tips</a></li>
</ul>
<p><strong>Agisoft PhotoScan Professional</strong></p>
<ul>
<li>recommended by many professionals</li>
<li>not free but has a 30 day trial (not for commercial purposes), and a very significant education discount</li>
</ul>
<h3 id="tips">Tips</h3>
<p>For shooting tips, <a href="https://www.youtube.com/watch?v=D7Torjkfec4">see this video</a>. Briefly:</p>
<ul>
<li>use the same camera settings and lighting for all images</li>
<li>use diffuse/flat lighting (avoid spots, choose overcast days when outdoors)</li>
<li>make sure nothing is moving and keep any backgrounds as plain as possible</li>
<li>avoid shiny and transparent objects</li>
<li>get lots of overlap between images... 40% image overlap, 10 degree angles, etc.</li>
</ul>
<p>Bear in mind that photogrammetry applications usually turn out very large files (high poly = high polygon count meshes), which need to be reduced <em>significantly</em> to keep a good framerate. For detailed workflows of bringing real objects into Unreal, there&#39;s a fantastically detailed set of articles on the Unreal blog, <a href="https://www.unrealengine.com/blog/imperfection-for-perfection">starting here</a>. For additional references, see these links:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=D6eqW6yk50k">Using only free tools</a></li>
<li><a href="https://www.youtube.com/watch?v=aemJKOMTRGQ">Creating a rock from photogrammetry</a></li>
</ul>
<h2 id="volumetric-capture-volumetric-video">Volumetric Capture / Volumetric Video</h2>
<p><img src="http://57.media.tumblr.com/d9412a062bcf9e58847ffb2db7990839/tumblr_nzhs49wQUy1qamt2wo1_500.gif" alt="depthkit"></p>
<p>The term <strong>Volumetric Capture</strong> (AKA Volumetric Video, Volumetric VR, etc.) has emerged very recently to describe a way of capturing time-varying parts of reality (typically actors) by combining multiple images/video signals captured at different locations in space, typically surrounding the subject, and often in combination with range cameras (cameras that can also determine depth, such as LIDAR cameras, or cheaper options such as Microsoft&#39;s Kinect). It is a kind of time-based photogrammetry, and produces a series of <strong>point clouds</strong> or a series of surface meshes, possibly also with colour data (textures).</p>
<p><a href="http://www.depthkit.tv">See for example DepthKit</a></p>
<iframe width="640" height="360" src="https://www.youtube.com/embed/g8m_LY_HcCo?rel=0" frameborder="0" allowfullscreen></iframe>

<h2 id="360-filmmaking">360 Filmmaking</h2>
<ul>
<li>&quot;Inside-out&quot; 360 camera based film-making<ul>
<li>Google Jump, JauntVR, VRSE/Within, <strong>many many</strong> others -- a huge boom in 360 cameras right now</li>
</ul>
</li>
<li>Many practical challenges<ul>
<li>where does the crew go?</li>
<li>actors are unaware of viewer</li>
<li>stitching &amp; stereo artifacts</li>
<li>doesn&#39;t allow head movement (but light-field cameras like <a href="https://www.lytro.com/immerge">Lytro Immerge</a>)?</li>
<li>still, massive interest &amp; investment!</li>
</ul>
</li>
<li><p><a href="http://making360.com">Various contributors (open source). Making 360. Github, ongoing.</a></p>
</li>
<li><p>Perhaps, forget what you knew about cinema?</p>
<ul>
<li>frame, focus, <a href="https://medium.com/the-language-of-vr/in-the-blink-of-a-mind-attention-1fdff60fa045#.61ahduc8u">attention</a></li>
<li>cuts &amp; <a href="https://vrfilmreview.ru/what-we-ve-learned-about-editing-in-vr360-905e32e9f9bd#.ghn2t04cc">editing in general</a></li>
</ul>
</li>
</ul>
<blockquote>
<p>&quot;The recent announcement of Vrse changing its name to Within is representative of an important shift in the way certain artists are thinking about VR. Chris Milk, one of the leading creators in the VR space is moving away from thinking about VR as a medium in which the author tells a story, and toward thinking about it as a medium in which viewers can, for the first time, step directly into the world of the creator.&quot; - <a href="http://uploadvr.com/chris-milk-storytelling-change-within/">source</a></p>
<p>&quot;VR eliminates the need for external frames. For the first time, the medium is no longer outside us, but within us. The paint is human experience and the canvas is our consciousness. The idea of an externalized medium ceases to exist. That’s why I think of VR as the last medium.&quot; - <a href="https://virtualrealitypop.com/futureofvr-8be30f0fca6a#.n1s3d4n92">source</a></p>
<p>&quot;Every rule we’ve discovered and employed in traditional filmmaking has an entirely new effect in VR. It’s very exciting. All the tools I’ve learned to love as a filmmaker now operate differently.&quot; - <a href="http://uploadvr.com/chris-milk-storytelling-change-within/">source</a></p>
</blockquote>
<p>Jessica Brillhart, Google&#39;s Principal Filmmaker for VR, gave a good introduction to the challenges posing filmmakers for cinematic VR at Google I/O 2016 – providing some of her own insights from being a creator in this field while highlighting experiences produced by the world&#39;s most prolific VR content creators:</p>
<iframe width="640" height="360" src="https://www.youtube.com/embed/t3xDgONMdlM?rel=0" frameborder="0" allowfullscreen></iframe>

<p><a href="https://medium.com/@brillhart">She has also posted widely on the Medium</a></p>
<ul>
<li><a href="https://virtualrealitypop.com/storyboarding-in-virtual-reality-67d3438a2fb1#.lwt8ojgeu">How to storyboard for VR/360</a> -- “Instead of controlling what the audience sees in VR, we work with probabilistic areas of user attention based on ergonomic data.”<ul>
<li><a href="https://www.youtube.com/watch?v=oW5RRolm9J8&amp;feature=youtu.be&amp;t=10m40s">VR at Dreamworks</a> -- Cylindrical concept with personal, action, and vista distances. Like close, medium and long shots, except that they all happen at the same time.
Stresses important of previz in VR as soon and as much as possible.</li>
</ul>
</li>
</ul>

	</div>
	<footer>&copy; 2016 Graham Wakefield</footer>
  </body>
</html>